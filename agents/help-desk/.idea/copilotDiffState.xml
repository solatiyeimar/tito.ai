<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/bots/base_bot.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/bots/base_bot.py" />
              <option name="originalContent" value="&quot;&quot;&quot;Base bot framework for shared functionality.&quot;&quot;&quot;&#10;&#10;from abc import ABC, abstractmethod&#10;from typing import Optional, List, Dict&#10;&#10;from pipecat.pipeline.task import PipelineParams, PipelineTask&#10;from pipecat.pipeline.pipeline import Pipeline&#10;from pipecat.pipeline.parallel_pipeline import ParallelPipeline&#10;from pipecat.pipeline.runner import PipelineRunner&#10;from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIProcessor&#10;from pipecat.processors.filters.function_filter import FunctionFilter&#10;from pipecat.services.deepgram.stt import DeepgramSTTService&#10;from pipecat.services.deepgram.tts import DeepgramTTSService&#10;from pipecat.services.cartesia.tts import CartesiaTTSService&#10;from pipecat.services.elevenlabs.tts import ElevenLabsTTSService&#10;from pipecat.services.google.llm import GoogleLLMService&#10;from pipecat.services.openai.llm import OpenAILLMService&#10;from pipecat.processors.aggregators.openai_llm_context import (&#10;    OpenAILLMContext,&#10;    OpenAILLMContextFrame,&#10;)&#10;from pipecat.processors.filters.stt_mute_filter import (&#10;    STTMuteFilter,&#10;    STTMuteConfig,&#10;    STTMuteStrategy,&#10;)&#10;from pipecat.services.rime.tts import RimeHttpTTSService&#10;from pipecat.transports.daily.transport import DailyTransport, DailyParams&#10;from pipecat.audio.vad.silero import SileroVADAnalyzer&#10;from pipecat.sync.event_notifier import EventNotifier&#10;from pipecat.processors.user_idle_processor import UserIdleProcessor&#10;from pipecat.frames.frames import (&#10;    UserStartedSpeakingFrame,&#10;    UserStoppedSpeakingFrame,&#10;    TranscriptionFrame,&#10;    LLMMessagesFrame,&#10;    StartInterruptionFrame,&#10;    FunctionCallInProgressFrame,&#10;    FunctionCallResultFrame,&#10;)&#10;from pipecat.transcriptions.language import Language  # Importado&#10;from pipecat.utils.text.markdown_text_filter import MarkdownTextFilter  # Importado&#10;&#10;from deepgram import LiveOptions&#10;&#10;from loguru import logger&#10;import time&#10;import types # Import types for SimpleNamespace&#10;&#10;from .smart_endpointing import (&#10;    CLASSIFIER_SYSTEM_INSTRUCTION,&#10;    CompletenessCheck,&#10;    OutputGate,&#10;    StatementJudgeContextFilter,&#10;)&#10;&#10;# Importamos la config&#10;from config.bot import BotConfig&#10;&#10;&#10;class BaseBot(ABC):&#10;    &quot;&quot;&quot;Abstract base class for bot implementations.&quot;&quot;&quot;&#10;&#10;    def __init__(self, config: BotConfig, system_messages: Optional[List[Dict[str, str]]] = None):&#10;        &quot;&quot;&quot;Initialize bot with core services and pipeline components.&#10;&#10;        Args:&#10;            config: Application configuration.&#10;            system_messages: Optional initial system messages for the LLM context.&#10;        &quot;&quot;&quot;&#10;        self.config = config&#10;&#10;        # Initialize STT service (basado en tu script)&#10;        self.stt = DeepgramSTTService(&#10;            api_key=config.deepgram_api_key,&#10;            live_options=LiveOptions(&#10;                language=&quot;multi&quot;,&#10;            ),&#10;            model=&quot;nova-2-general&quot;,&#10;            language=Language.ES  # Usar enum&#10;        )&#10;&#10;        # Initialize TTS service (config-driven)&#10;        match config.tts_provider:&#10;            case &quot;elevenlabs&quot;:&#10;                if not config.elevenlabs_api_key:&#10;                    raise ValueError(&quot;ElevenLabs API key is required for ElevenLabs TTS&quot;)&#10;&#10;                self.tts = ElevenLabsTTSService(&#10;                    api_key=config.elevenlabs_api_key,&#10;                    voice_id=config.elevenlabs_voice_id,&#10;                    model=&quot;eleven_multilingual_v2&quot;,  # De tu script&#10;                    text_filters=[MarkdownTextFilter()],  # De tu script&#10;                )&#10;            case &quot;cartesia&quot;:&#10;                if not config.cartesia_api_key:&#10;                    raise ValueError(&quot;Cartesia API key is required for Cartesia TTS&quot;)&#10;&#10;                self.tts = CartesiaTTSService(&#10;                    api_key=config.cartesia_api_key,&#10;                    voice_name=config.cartesia_voice,  # &quot;CO-M-1&quot; de tu script&#10;                    model_id=&quot;sonic-es&quot;,  # De tu script&#10;                    text_filters=[MarkdownTextFilter()],  # De tu script&#10;                )&#10;            case &quot;deepgram&quot;:&#10;                if not config.deepgram_api_key:&#10;                    raise ValueError(&quot;Deepgram API key is required for Deepgram TTS&quot;)&#10;&#10;                self.tts = DeepgramTTSService(&#10;                    api_key=config.deepgram_api_key, voice=config.deepgram_voice&#10;                )&#10;            case &quot;rime&quot;:&#10;                if not config.rime_api_key:&#10;                    raise ValueError(&quot;Rime API key is required for Rime TTS&quot;)&#10;&#10;                self.tts = RimeHttpTTSService(&#10;                    api_key=config.rime_api_key,&#10;                    voice_id=config.rime_voice_id,&#10;                    params=RimeHttpTTSService.InputParams(&#10;                        reduce_latency=config.rime_reduce_latency,&#10;                        speed_alpha=config.rime_speed_alpha,&#10;                    ),&#10;                )&#10;            case _:&#10;                raise ValueError(f&quot;Invalid TTS provider: {config.tts_provider}&quot;)&#10;&#10;        # Initialize LLM services&#10;        match config.llm_provider:&#10;            case &quot;google&quot;:&#10;                if not config.google_api_key:&#10;                    raise ValueError(&quot;Google API key is required for Google LLM&quot;)&#10;&#10;                system_instruction = (&#10;                    system_messages[0][&quot;content&quot;]&#10;                    if system_messages&#10;                    else &quot;Eres un asistente de voz&quot;  # Default genérico&#10;                )&#10;                self.conversation_llm = GoogleLLMService(&#10;                    api_key=config.google_api_key,&#10;                    model=config.google_model,&#10;                    params=config.google_params,&#10;                    system_instruction=system_instruction,&#10;                )&#10;                # Manually add a config attribute with custom_data for FlowManager compatibility&#10;                self.conversation_llm.config = types.SimpleNamespace(custom_data={})&#10;                self.llm = self.conversation_llm  # Alias&#10;&#10;                # Statement classifier LLM for endpoint detection&#10;                self.statement_llm = GoogleLLMService(&#10;                    name=&quot;StatementJudger&quot;,&#10;                    api_key=config.google_api_key,&#10;                    model=config.classifier_model,&#10;                    temperature=0.0,&#10;                    system_instruction=CLASSIFIER_SYSTEM_INSTRUCTION,&#10;                )&#10;                # Manually add a config attribute with custom_data for FlowManager compatibility&#10;                self.statement_llm.config = types.SimpleNamespace(custom_data={})&#10;&#10;            case &quot;openai&quot;:&#10;                if not config.openai_api_key:&#10;                    raise ValueError(&quot;OpenAI API key is required for OpenAI LLM&quot;)&#10;&#10;                self.conversation_llm = OpenAILLMService(&#10;                    api_key=config.openai_api_key,&#10;                    model=config.openai_model,&#10;                    params=config.openai_params,&#10;                    messages=system_messages  # OpenAI usa 'messages'&#10;                )&#10;                self.llm = self.conversation_llm  # Alias&#10;&#10;                # Smart endpointing no está implementado para OpenAI en esta base&#10;                logger.warning(&quot;Smart endpointing (StatementJudger) is not implemented for OpenAI. Using standard VAD.&quot;)&#10;                self.statement_llm = None&#10;&#10;            case _:&#10;                # Aquí podríamos integrar el `create_llm()` del `utils.py`&#10;                # pero por ahora seguimos el patrón de la base de código.&#10;                raise ValueError(f&quot;Invalid LLM provider: {config.llm_provider}&quot;)&#10;&#10;        # Initialize context&#10;        self.context = OpenAILLMContext(messages=system_messages)&#10;        self.context_aggregator = self.conversation_llm.create_context_aggregator(self.context)&#10;&#10;        # Initialize mute filter&#10;        self.stt_mute_filter = (&#10;            STTMuteFilter(&#10;                stt_service=self.stt,&#10;                config=STTMuteConfig(&#10;                    strategies={&#10;                        STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE,&#10;                        STTMuteStrategy.FUNCTION_CALL,&#10;                    }&#10;                ),&#10;            )&#10;            if config.enable_stt_mute_filter&#10;            else None&#10;        )&#10;&#10;        logger.debug(f&quot;Initialised bot with config: {config}&quot;)&#10;&#10;        # Initialize transport params (de tu script)&#10;        self.transport_params = DailyParams(&#10;            audio_in_enabled=True,&#10;            audio_out_enabled=True,&#10;            vad_analyzer=SileroVADAnalyzer(),&#10;            # vad_audio_passthrough=True, # BaseBot usa esto, tu script no. Lo dejamos.&#10;        )&#10;&#10;        # Initialize RTVI with default config&#10;        self.rtvi = RTVIProcessor(config=RTVIConfig(config=[]))&#10;&#10;        # Initialize smart endpointing components (si aplica)&#10;        if self.statement_llm:&#10;            self.notifier = EventNotifier()&#10;            self.statement_judge_context_filter = StatementJudgeContextFilter(notifier=self.notifier)&#10;            self.completeness_check = CompletenessCheck(notifier=self.notifier)&#10;            self.output_gate = OutputGate(notifier=self.notifier, start_open=True)&#10;&#10;            async def user_idle_notifier(frame):&#10;                await self.notifier.notify()&#10;&#10;            self.user_idle = UserIdleProcessor(callback=user_idle_notifier, timeout=5.0)&#10;        else:&#10;            # Si no hay statement_llm, usamos un UserIdleProcessor simple&#10;            self.user_idle = UserIdleProcessor(timeout=5.0)  # Callback no necesario&#10;            self.statement_judge_context_filter = None&#10;            self.completeness_check = None&#10;            self.output_gate = None&#10;            self.notifier = None&#10;&#10;        # These will be set up when needed&#10;        self.transport: Optional[DailyTransport] = None&#10;        self.task: Optional[PipelineTask] = None&#10;        self.runner: Optional[PipelineRunner] = None&#10;&#10;    async def setup_transport(self, url: str, token: str):&#10;        &quot;&quot;&quot;Set up the transport with the given URL and token.&quot;&quot;&quot;&#10;        self.transport = DailyTransport(url, token, self.config.bot_name, self.transport_params)&#10;&#10;        # Set up basic event handlers&#10;        @self.transport.event_handler(&quot;on_participant_left&quot;)&#10;        async def on_participant_left(transport, participant, reason):&#10;            logger.warning(f&quot;Participante {participant.get('user_name')} se fue: {reason}&quot;)&#10;            # El bot base no cancela aquí, pero tu handler sí.&#10;            # Lo dejamos por ahora, pero considera si quieres que 1 solo usuario&#10;            # al irse termine la llamada.&#10;            # if self.task:&#10;            #     await self.task.cancel()&#10;&#10;        @self.transport.event_handler(&quot;on_client_connected&quot;)&#10;        async def on_client_connected(transport, client):&#10;            # Este evento es de pipecat-flows, el BaseBot usa on_first_participant_joined&#10;            # El handler real se moverá a FlowBot&#10;            logger.info(f&quot;Cliente (estudiante) conectado: {client}&quot;)&#10;            await self._handle_first_participant()&#10;&#10;        @self.transport.event_handler(&quot;on_client_disconnected&quot;)&#10;        async def on_client_disconnected(transport, client):&#10;            logger.info(f&quot;Cliente (estudiante) desconectado: {client}&quot;)&#10;            if self.task:&#10;                await self.task.cancel()&#10;&#10;        @self.transport.event_handler(&quot;on_first_participant_joined&quot;)&#10;        async def on_first_participant_joined(transport, participant):&#10;            logger.info(f&quot;Primer participante unido: {participant.get('user_name')}&quot;)&#10;            await transport.capture_participant_transcription(participant[&quot;id&quot;])&#10;            # await self._handle_first_participant() # Se llama en on_client_connected&#10;&#10;        @self.transport.event_handler(&quot;on_app_message&quot;)&#10;        async def on_app_message(transport, message, sender):&#10;            if &quot;message&quot; not in message:&#10;                return&#10;&#10;            await self.task.queue_frames(&#10;                [&#10;                    UserStartedSpeakingFrame(),&#10;                    TranscriptionFrame(&#10;                        user_id=sender, timestamp=time.time(), text=message[&quot;message&quot;]&#10;                    ),&#10;                    UserStoppedSpeakingFrame(),&#10;                ]&#10;            )&#10;&#10;    def create_pipeline(self):&#10;        &quot;&quot;&quot;Create the processing pipeline.&quot;&quot;&quot;&#10;        if not self.transport:&#10;            raise RuntimeError(&quot;Transport must be set up before creating pipeline&quot;)&#10;&#10;        # async def block_user_stopped_speaking(frame):&#10;        #     return not isinstance(frame, UserStoppedSpeakingFrame)&#10;        #&#10;        # async def pass_only_llm_trigger_frames(frame):&#10;        #     return (&#10;        #         isinstance(frame, OpenAILLMContextFrame)&#10;        #         or isinstance(frame, LLMMessagesFrame)&#10;        #         or isinstance(frame, StartInterruptionFrame)&#10;        #         or isinstance(frame, StopInterruptionFrame)&#10;        #         or isinstance(frame, FunctionCallInProgressFrame)&#10;        #         or isinstance(frame, FunctionCallResultFrame)&#10;        #     )&#10;        #&#10;        # # Define an async filter that always discards frames.&#10;        # async def discard_all(frame):&#10;        #     return False&#10;&#10;        # Build pipeline with Deepgram STT at the beginning&#10;        pipeline = Pipeline(&#10;            [&#10;                processor&#10;                for processor in [&#10;                    self.rtvi,&#10;                    self.transport.input(),&#10;                    self.stt_mute_filter,&#10;                    self.stt,  # Deepgram transcribes incoming audio&#10;                    self.context_aggregator.user(),&#10;                    # ParallelPipeline(&#10;                    #     [&#10;                    #         # Branch 1: Pass everything except UserStoppedSpeakingFrame&#10;                    #         FunctionFilter(filter=block_user_stopped_speaking),&#10;                    #     ],&#10;                    #     [&#10;                    #         # Branch 2: Endpoint detection branch using Gemini for completeness&#10;                    #         self.statement_judge_context_filter,&#10;                    #         self.statement_llm,&#10;                    #         self.completeness_check,&#10;                    #         # Use an async filter to discard branch 2's output.&#10;                    #         FunctionFilter(filter=discard_all),&#10;                    #     ],&#10;                    #     [&#10;                    #         # Branch 3: Conversation branch using Gemini for dialogue&#10;                    #         FunctionFilter(filter=pass_only_llm_trigger_frames),&#10;                    #         self.conversation_llm,&#10;                    #         self.output_gate,&#10;                    #     ],&#10;                    # ),&#10;                    self.conversation_llm,&#10;                    self.tts,&#10;                    self.user_idle,&#10;                    self.transport.output(),&#10;                    self.context_aggregator.assistant(),&#10;                ]&#10;                if processor is not None&#10;            ]&#10;        )&#10;&#10;        self.task = PipelineTask(&#10;            pipeline,&#10;            PipelineParams(&#10;                allow_interruptions=True,&#10;                enable_metrics=True,&#10;                enable_usage_metrics=True,&#10;            ),&#10;        )&#10;        self.runner = PipelineRunner()&#10;&#10;    async def start(self):&#10;        &quot;&quot;&quot;Start the bot's main task.&quot;&quot;&quot;&#10;        if not self.runner or not self.task:&#10;            raise RuntimeError(&quot;Bot not properly initialized. Call create_pipeline first.&quot;)&#10;        await self.runner.run(self.task)&#10;&#10;    async def cleanup(self):&#10;        &quot;&quot;&quot;Clean up resources.&quot;&quot;&quot;&#10;        if self.runner:&#10;            await self.runner.stop_when_done()&#10;        if self.transport:&#10;            await self.transport.close()&#10;&#10;    @abstractmethod&#10;    async def _handle_first_participant(self):&#10;        &quot;&quot;&quot;Override in subclass to handle the first participant joining.&quot;&quot;&quot;&#10;        pass&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;Base bot framework for shared functionality.&quot;&quot;&quot;&#10;&#10;from abc import ABC, abstractmethod&#10;from typing import Optional, List, Dict&#10;&#10;from pipecat.pipeline.task import PipelineParams, PipelineTask&#10;from pipecat.pipeline.pipeline import Pipeline&#10;from pipecat.pipeline.parallel_pipeline import ParallelPipeline&#10;from pipecat.pipeline.runner import PipelineRunner&#10;from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIProcessor&#10;from pipecat.processors.filters.function_filter import FunctionFilter&#10;from pipecat.services.deepgram.stt import DeepgramSTTService&#10;from pipecat.services.deepgram.tts import DeepgramTTSService&#10;from pipecat.services.cartesia.tts import CartesiaTTSService&#10;from pipecat.services.elevenlabs.tts import ElevenLabsTTSService&#10;from pipecat.services.google.llm import GoogleLLMService&#10;from pipecat.services.openai.llm import OpenAILLMService&#10;from pipecat.processors.aggregators.openai_llm_context import (&#10;    OpenAILLMContext,&#10;    OpenAILLMContextFrame,&#10;)&#10;from pipecat.processors.filters.stt_mute_filter import (&#10;    STTMuteFilter,&#10;    STTMuteConfig,&#10;    STTMuteStrategy,&#10;)&#10;from pipecat.services.rime.tts import RimeHttpTTSService&#10;from pipecat.transports.daily.transport import DailyTransport, DailyParams&#10;from pipecat.audio.vad.silero import SileroVADAnalyzer&#10;from pipecat.sync.event_notifier import EventNotifier&#10;from pipecat.processors.user_idle_processor import UserIdleProcessor&#10;from pipecat.frames.frames import (&#10;    UserStartedSpeakingFrame,&#10;    UserStoppedSpeakingFrame,&#10;    TranscriptionFrame,&#10;    LLMMessagesFrame,&#10;    StartInterruptionFrame,&#10;    FunctionCallInProgressFrame,&#10;    FunctionCallResultFrame,&#10;)&#10;from pipecat.transcriptions.language import Language  # Importado&#10;from pipecat.utils.text.markdown_text_filter import MarkdownTextFilter  # Importado&#10;&#10;from deepgram import LiveOptions&#10;&#10;from loguru import logger&#10;import time&#10;import types # Import types for SimpleNamespace&#10;&#10;from .smart_endpointing import (&#10;    CLASSIFIER_SYSTEM_INSTRUCTION,&#10;    CompletenessCheck,&#10;    OutputGate,&#10;    StatementJudgeContextFilter,&#10;)&#10;&#10;# Importamos la config&#10;from config.bot import BotConfig&#10;&#10;&#10;class BaseBot(ABC):&#10;    &quot;&quot;&quot;Abstract base class for bot implementations.&quot;&quot;&quot;&#10;&#10;    def __init__(self, config: BotConfig, system_messages: Optional[List[Dict[str, str]]] = None):&#10;        &quot;&quot;&quot;Initialize bot with core services and pipeline components.&#10;&#10;        Args:&#10;            config: Application configuration.&#10;            system_messages: Optional initial system messages for the LLM context.&#10;        &quot;&quot;&quot;&#10;        self.config = config&#10;&#10;        # Initialize STT service (basado en tu script)&#10;        self.stt = DeepgramSTTService(&#10;            api_key=config.deepgram_api_key,&#10;            live_options=LiveOptions(&#10;                language=&quot;multi&quot;,&#10;            ),&#10;            model=&quot;nova-2-general&quot;,&#10;            language=Language.ES  # Usar enum&#10;        )&#10;&#10;        # Initialize TTS service (config-driven)&#10;        match config.tts_provider:&#10;            case &quot;elevenlabs&quot;:&#10;                if not config.elevenlabs_api_key:&#10;                    raise ValueError(&quot;ElevenLabs API key is required for ElevenLabs TTS&quot;)&#10;&#10;                self.tts = ElevenLabsTTSService(&#10;                    api_key=config.elevenlabs_api_key,&#10;                    voice_id=config.elevenlabs_voice_id,&#10;                    model=&quot;eleven_multilingual_v2&quot;,  # De tu script&#10;                    text_filters=[MarkdownTextFilter()],  # De tu script&#10;                )&#10;            case &quot;cartesia&quot;:&#10;                if not config.cartesia_api_key:&#10;                    raise ValueError(&quot;Cartesia API key is required for Cartesia TTS&quot;)&#10;&#10;                self.tts = CartesiaTTSService(&#10;                    api_key=config.cartesia_api_key,&#10;                    voice_name=config.cartesia_voice,  # &quot;CO-M-1&quot; de tu script&#10;                    model_id=&quot;sonic-es&quot;,  # De tu script&#10;                    text_filters=[MarkdownTextFilter()],  # De tu script&#10;                )&#10;            case &quot;deepgram&quot;:&#10;                if not config.deepgram_api_key:&#10;                    raise ValueError(&quot;Deepgram API key is required for Deepgram TTS&quot;)&#10;&#10;                self.tts = DeepgramTTSService(&#10;                    api_key=config.deepgram_api_key, voice=config.deepgram_voice&#10;                )&#10;            case &quot;rime&quot;:&#10;                if not config.rime_api_key:&#10;                    raise ValueError(&quot;Rime API key is required for Rime TTS&quot;)&#10;&#10;                self.tts = RimeHttpTTSService(&#10;                    api_key=config.rime_api_key,&#10;                    voice_id=config.rime_voice_id,&#10;                    params=RimeHttpTTSService.InputParams(&#10;                        reduce_latency=config.rime_reduce_latency,&#10;                        speed_alpha=config.rime_speed_alpha,&#10;                    ),&#10;                )&#10;            case _:&#10;                raise ValueError(f&quot;Invalid TTS provider: {config.tts_provider}&quot;)&#10;&#10;        # Initialize LLM services&#10;        match config.llm_provider:&#10;            case &quot;google&quot;:&#10;                if not config.google_api_key:&#10;                    raise ValueError(&quot;Google API key is required for Google LLM&quot;)&#10;&#10;                system_instruction = (&#10;                    system_messages[0][&quot;content&quot;]&#10;                    if system_messages&#10;                    else &quot;Eres un asistente de voz&quot;  # Default genérico&#10;                )&#10;                self.conversation_llm = GoogleLLMService(&#10;                    api_key=config.google_api_key,&#10;                    model=config.google_model,&#10;                    params=config.google_params,&#10;                    system_instruction=system_instruction,&#10;                )&#10;                # Manually add a config attribute with custom_data for FlowManager compatibility&#10;                self.conversation_llm.config = types.SimpleNamespace(custom_data={})&#10;                self.llm = self.conversation_llm  # Alias&#10;&#10;                # Statement classifier LLM for endpoint detection&#10;                self.statement_llm = GoogleLLMService(&#10;                    name=&quot;StatementJudger&quot;,&#10;                    api_key=config.google_api_key,&#10;                    model=config.classifier_model,&#10;                    temperature=0.0,&#10;                    system_instruction=CLASSIFIER_SYSTEM_INSTRUCTION,&#10;                )&#10;                # Manually add a config attribute with custom_data for FlowManager compatibility&#10;                self.statement_llm.config = types.SimpleNamespace(custom_data={})&#10;&#10;            case &quot;openai&quot;:&#10;                if not config.openai_api_key:&#10;                    raise ValueError(&quot;OpenAI API key is required for OpenAI LLM&quot;)&#10;&#10;                self.conversation_llm = OpenAILLMService(&#10;                    api_key=config.openai_api_key,&#10;                    model=config.openai_model,&#10;                    params=config.openai_params,&#10;                    messages=system_messages  # OpenAI usa 'messages'&#10;                )&#10;                self.llm = self.conversation_llm  # Alias&#10;&#10;                # Smart endpointing no está implementado para OpenAI en esta base&#10;                logger.warning(&quot;Smart endpointing (StatementJudger) is not implemented for OpenAI. Using standard VAD.&quot;)&#10;                self.statement_llm = None&#10;&#10;            case _:&#10;                # Aquí podríamos integrar el `create_llm()` del `utils.py`&#10;                # pero por ahora seguimos el patrón de la base de código.&#10;                raise ValueError(f&quot;Invalid LLM provider: {config.llm_provider}&quot;)&#10;&#10;        # Initialize context&#10;        self.context = OpenAILLMContext(messages=system_messages)&#10;        self.context_aggregator = self.conversation_llm.create_context_aggregator(self.context)&#10;&#10;        # Initialize mute filter&#10;        self.stt_mute_filter = (&#10;            STTMuteFilter(&#10;                stt_service=self.stt,&#10;                config=STTMuteConfig(&#10;                    strategies={&#10;                        STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE,&#10;                        STTMuteStrategy.FUNCTION_CALL,&#10;                    }&#10;                ),&#10;            )&#10;            if config.enable_stt_mute_filter&#10;            else None&#10;        )&#10;&#10;        logger.debug(f&quot;Initialised bot with config: {config}&quot;)&#10;&#10;        # Initialize transport params (de tu script)&#10;        self.transport_params = DailyParams(&#10;            audio_in_enabled=True,&#10;            audio_out_enabled=True,&#10;            vad_analyzer=SileroVADAnalyzer(),&#10;            # vad_audio_passthrough=True, # BaseBot usa esto, tu script no. Lo dejamos.&#10;        )&#10;&#10;        # Initialize RTVI with default config&#10;        self.rtvi = RTVIProcessor(config=RTVIConfig(config=[]))&#10;&#10;        # Initialize smart endpointing components (si aplica)&#10;        if self.statement_llm:&#10;            self.notifier = EventNotifier()&#10;            self.statement_judge_context_filter = StatementJudgeContextFilter(notifier=self.notifier)&#10;            self.completeness_check = CompletenessCheck(notifier=self.notifier)&#10;            self.output_gate = OutputGate(notifier=self.notifier, start_open=True)&#10;&#10;            async def user_idle_notifier(frame):&#10;                await self.notifier.notify()&#10;&#10;            self.user_idle = UserIdleProcessor(callback=user_idle_notifier, timeout=5.0)&#10;        else:&#10;            # Si no hay statement_llm, usamos un UserIdleProcessor simple&#10;            self.user_idle = UserIdleProcessor(timeout=5.0)  # Callback no necesario&#10;            self.statement_judge_context_filter = None&#10;            self.completeness_check = None&#10;            self.output_gate = None&#10;            self.notifier = None&#10;&#10;        # These will be set up when needed&#10;        self.transport: Optional[DailyTransport] = None&#10;        self.task: Optional[PipelineTask] = None&#10;        self.runner: Optional[PipelineRunner] = None&#10;&#10;    async def setup_transport(self, url: str, token: str):&#10;        &quot;&quot;&quot;Set up the transport with the given URL and token.&quot;&quot;&quot;&#10;        self.transport = DailyTransport(url, token, self.config.bot_name, self.transport_params)&#10;&#10;        # Set up basic event handlers&#10;        @self.transport.event_handler(&quot;on_participant_left&quot;)&#10;        async def on_participant_left(transport, participant, reason):&#10;            logger.warning(f&quot;Participante {participant.get('user_name')} se fue: {reason}&quot;)&#10;            # El bot base no cancela aquí, pero tu handler sí.&#10;            # Lo dejamos por ahora, pero considera si quieres que 1 solo usuario&#10;            # al irse termine la llamada.&#10;            # if self.task:&#10;            #     await self.task.cancel()&#10;&#10;        @self.transport.event_handler(&quot;on_client_connected&quot;)&#10;        async def on_client_connected(transport, client):&#10;            # Este evento es de pipecat-flows, el BaseBot usa on_first_participant_joined&#10;            # El handler real se moverá a FlowBot&#10;            logger.info(f&quot;Cliente (estudiante) conectado: {client}&quot;)&#10;            await self._handle_first_participant()&#10;&#10;        @self.transport.event_handler(&quot;on_client_disconnected&quot;)&#10;        async def on_client_disconnected(transport, client):&#10;            logger.info(f&quot;Cliente (estudiante) desconectado: {client}&quot;)&#10;            if self.task:&#10;                await self.task.cancel()&#10;&#10;        @self.transport.event_handler(&quot;on_first_participant_joined&quot;)&#10;        async def on_first_participant_joined(transport, participant):&#10;            logger.info(f&quot;Primer participante unido: {participant.get('user_name')}&quot;)&#10;            await transport.capture_participant_transcription(participant[&quot;id&quot;])&#10;            # await self._handle_first_participant() # Se llama en on_client_connected&#10;&#10;        @self.transport.event_handler(&quot;on_app_message&quot;)&#10;        async def on_app_message(transport, message, sender):&#10;            if &quot;message&quot; not in message:&#10;                return&#10;&#10;            await self.task.queue_frames(&#10;                [&#10;                    UserStartedSpeakingFrame(),&#10;                    TranscriptionFrame(&#10;                        user_id=sender, timestamp=time.time(), text=message[&quot;message&quot;]&#10;                    ),&#10;                    UserStoppedSpeakingFrame(),&#10;                ]&#10;            )&#10;&#10;    def create_pipeline(self):&#10;        &quot;&quot;&quot;Create the processing pipeline.&quot;&quot;&quot;&#10;        if not self.transport:&#10;            raise RuntimeError(&quot;Transport must be set up before creating pipeline&quot;)&#10;&#10;        # async def block_user_stopped_speaking(frame):&#10;        #     return not isinstance(frame, UserStoppedSpeakingFrame)&#10;        #&#10;        # async def pass_only_llm_trigger_frames(frame):&#10;        #     return (&#10;        #         isinstance(frame, OpenAILLMContextFrame)&#10;        #         or isinstance(frame, LLMMessagesFrame)&#10;        #         or isinstance(frame, StartInterruptionFrame)&#10;        #         or isinstance(frame, StopInterruptionFrame)&#10;        #         or isinstance(frame, FunctionCallInProgressFrame)&#10;        #         or isinstance(frame, FunctionCallResultFrame)&#10;        #     )&#10;        #&#10;        # # Define an async filter that always discards frames.&#10;        # async def discard_all(frame):&#10;        #     return False&#10;&#10;        # Build pipeline with Deepgram STT at the beginning&#10;        pipeline = Pipeline(&#10;            [&#10;                processor&#10;                for processor in [&#10;                    self.rtvi,&#10;                    self.transport.input(),&#10;                    self.stt_mute_filter,&#10;                    self.stt,  # Deepgram transcribes incoming audio&#10;                    self.context_aggregator.user(),&#10;                    # ParallelPipeline(&#10;                    #     [&#10;                    #         # Branch 1: Pass everything except UserStoppedSpeakingFrame&#10;                    #         FunctionFilter(filter=block_user_stopped_speaking),&#10;                    #     ],&#10;                    #     [&#10;                    #         # Branch 2: Endpoint detection branch using Gemini for completeness&#10;                    #         self.statement_judge_context_filter,&#10;                    #         self.statement_llm,&#10;                    #         self.completeness_check,&#10;                    #         # Use an async filter to discard branch 2's output.&#10;                    #         FunctionFilter(filter=discard_all),&#10;                    #     ],&#10;                    #     [&#10;                    #         # Branch 3: Conversation branch using Gemini for dialogue&#10;                    #         FunctionFilter(filter=pass_only_llm_trigger_frames),&#10;                    #         self.conversation_llm,&#10;                    #         self.output_gate,&#10;                    #     ],&#10;                    # ),&#10;                    self.conversation_llm,&#10;                    self.tts,&#10;                    self.user_idle,&#10;                    self.transport.output(),&#10;                    self.context_aggregator.assistant(),&#10;                ]&#10;                if processor is not None&#10;            ]&#10;        )&#10;&#10;        self.task = PipelineTask(&#10;            pipeline,&#10;            params=PipelineParams(&#10;                allow_interruptions=True,&#10;                enable_metrics=True,&#10;                enable_usage_metrics=True,&#10;            ),&#10;        )&#10;        self.runner = PipelineRunner()&#10;&#10;    async def start(self):&#10;        &quot;&quot;&quot;Start the bot's main task.&quot;&quot;&quot;&#10;        if not self.runner or not self.task:&#10;            raise RuntimeError(&quot;Bot not properly initialized. Call create_pipeline first.&quot;)&#10;        await self.runner.run(self.task)&#10;&#10;    async def cleanup(self):&#10;        &quot;&quot;&quot;Clean up resources.&quot;&quot;&quot;&#10;        if self.runner:&#10;            await self.runner.stop_when_done()&#10;        if self.transport:&#10;            await self.transport.disconnect()&#10;&#10;    @abstractmethod&#10;    async def _handle_first_participant(self):&#10;        &quot;&quot;&quot;Override in subclass to handle the first participant joining.&quot;&quot;&quot;&#10;        pass&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>